{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redes Generativas Aplicadas a la Geografía Colombiana\n",
    "#### Danny Garzon degarzonm@unal.edu.co, Universidad Nacional de Colombia\n",
    "#### Noviembre 2023\n",
    "\n",
    "#### Introducción\n",
    "El presente notebook tiene como objetivo la implementación de una red generativa de adversarios (GAN) para la generación de mapas de la geografía colombiana. Para ello se utilizarán diversas librerias, estableceremos unas variables globales para el control del modelo generativo y otros aspectos para aprovechar el uso de la GPU.Luego haremos uso de técnicas de procsamiento de imagenes para establecer nuestro conjunto de datos, estableceremos el modelo y la arquitectura del sistema de redes neuronales, entrenaremos el modelo y finalmente evaluaremos los resultados obtenidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerias\n",
    "\n",
    "Realiza la importación de bibliotecas esenciales para el trabajo con Deep Learning, como TensorFlow, NumPy, y Matplotlib. También incluye comandos para verificar la disponibilidad de la GPU, listar dispositivos disponibles y comprobar las versiones de las bibliotecas importantes, asegurando un entorno adecuado para el procesamiento y análisis de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pydot\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, LeakyReLU, ReLU, BatchNormalization, Dropout, Concatenate\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# comprobamos que estamos usando la GPU\n",
    "print(\"GPU\", \"disponible\" if tf.config.list_physical_devices(\"GPU\") else \"No disponible\")\n",
    "# comprobamos que dispositivos tenemos disponibles\n",
    "print(tf.config.list_physical_devices())\n",
    "\n",
    "# comprobamos que version de cada libreria estamos usando\n",
    "print(\"version Python:\", sys.version)\n",
    "print(\"version Tensorflow: \", tf.__version__)\n",
    "print(\"version Numpy:\", np.__version__)\n",
    "print(\"version Matplotlib: \", plt.matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Parámetros y Rutas para el Entrenamiento del Modelo\n",
    "Esta celda establece variables globales clave para el entrenamiento del modelo. Incluye parámetros como el tamaño del buffer, tamaño del lote, dimensiones de las imágenes, semilla aleatoria, canales de salida, factor de regularización, número de épocas, configuración de autoguardado, y rutas para el dataset y checkpoints. Estos parámetros son fundamentales para configurar y controlar el proceso de entrenamiento y manejo de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variables globales\n",
    "TAM_BUFFER = 8000                 # tamaño del dataset a utilizar\n",
    "TAM_BATCH = 1                     # tamaño del lote\n",
    "IMG_ANCHO = 256                   # ancho de la imagen\n",
    "IMG_ALTO = 256                    # alto de la imagen\n",
    "SEMILLA = 2023                    # semilla para la generacion de numeros aleatorios\n",
    "CANALES_SALIDA = 3                # canales de salida de la imagen rgb\n",
    "LAMBDA = 100                      # factor de regularizacion \n",
    "EPOCAS = 200                       # epocas del entrenamiento\n",
    "AUTOGUARDADO_EPOCAS = 40           # epocas para autoguardar los checkpoints del entrenamiento\n",
    "RANGO_NORMALIZACION = (-1, 1)     # rango de normalizacion de las imagenes de [0,255] A [-1,1]\n",
    "\n",
    "RUTA_DATASET = 'dataset' # ruta del dataset relativa al notebook\n",
    "RUTA_PANEL = RUTA_DATASET + '/panel_' # ruta del panel relativa al notebook\n",
    "RUTA_IMG_PRUEBA = RUTA_DATASET + '/test_img' # ruta de la imagen de prueba relativa al notebook\n",
    "RUTA_CHECKPOINTS = RUTA_DATASET + '/checkpoints' # ruta de los checkpoints relativa al notebook\n",
    "RUTA_CHECKPOINTS_EXTERNO = 'CHECKPOINTS' # ruta de los checkpoints relativa al notebook\n",
    "PREFIJO_CHECKPOINT = RUTA_CHECKPOINTS+ \"/ckpt\"  # prefijo de los checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Datos para Entrenamiento y Prueba\n",
    "Esta celda se enfoca en la preparación de los datos para el entrenamiento y prueba. Inicialmente, lista los nombres de las imágenes en un directorio específico y calcula su cantidad total. Luego, separa las imágenes en conjuntos de entrenamiento y prueba, utilizando un porcentaje predefinido del total. Finalmente, imprime en pantalla el número de imágenes en cada conjunto, asegurando una distribución adecuada para el entrenamiento y la evaluación del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# listamos los nombres de las imagenes disponibles en el directorio para el entrenamiento\n",
    "LISTA_TOTAL_NOMBRES = os.listdir(RUTA_PANEL+'0') # lista de nombres de imagenes panel 0 representa la informacion geografica\n",
    "\n",
    "# comprobamos el numero de imagenes encontradas\n",
    "N_IMGS_EN_TOTAL = len(LISTA_TOTAL_NOMBRES)\n",
    "print(\"Número de imagenes encontradas:\", N_IMGS_EN_TOTAL)\n",
    "\n",
    "# numero de imagenes para el test\n",
    "# elegimos el 20% de las imagenes para el test, dejando 80% para el entrenamiento\n",
    "TAM_BUFFER_ENTRENAMIENTO = int(TAM_BUFFER * 0.8)\n",
    "TAM_BUFFER_PRUEBA = int(TAM_BUFFER * 0.2)\n",
    "# fijamos la semilla para la generacion de numeros aleatorios\n",
    "np.random.seed(SEMILLA)\n",
    "# barajamos la lista de nombres de imagenes\n",
    "np.random.shuffle(LISTA_TOTAL_NOMBRES)\n",
    "\n",
    "# particiones de imagenes para el entrenamiento y test\n",
    "LISTA_NMS_ENTRENAMIENTO = LISTA_TOTAL_NOMBRES[0:TAM_BUFFER_ENTRENAMIENTO]\n",
    "LISTA_NMS_PRUEBA = LISTA_TOTAL_NOMBRES[TAM_BUFFER_ENTRENAMIENTO:TAM_BUFFER]\n",
    "\n",
    "# escribimos en pantalla las dimensiones de los conjuntos de entrenamiento y test\n",
    "print(\"Numero de imagenes de entrenamiento \", len(LISTA_NMS_ENTRENAMIENTO))\n",
    "print(\"Numero de imagenes de prueba \", len(LISTA_NMS_PRUEBA))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Funciones Auxiliares para el Preprocesamiento de Imágenes\n",
    "Esta celda define funciones auxiliares para el preprocesamiento de imágenes. La función nombre_img_a_tensores carga y procesa imágenes, convirtiéndolas en tensores adecuados para el entrenamiento de modelos de Deep Learning. Incluye pasos como la decodificación de imágenes PNG, redimensionamiento, y normalización. Las funciones cambiar_tam y norm_tensor son utilizadas para cambiar el tamaño y normalizar los tensores respectivamente, facilitando su uso posterior en el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# funciones auxiliares para el preprocesado de las imagenes\n",
    "def nombre_img_a_tensores(nombre_img , paneles = 2, img_ancho = 256, img_alto = 256):\n",
    "    imgs_cargadas = []\n",
    "    # cargamos las imagenes\n",
    "    for x in range(paneles):\n",
    "        # cargamos las imagenes \n",
    "        archivo_img = tf.io.read_file(RUTA_PANEL + f\"{x}/\" + nombre_img)\n",
    "        # decodificamos las imagenes en formato png y las convertimos a tensores float32, teniendo en cuenta los 3 canales de color\n",
    "        tensor_panel_x = tf.cast(tf.image.decode_png(archivo_img),\n",
    "                           tf.float32)[..., :3]\n",
    "        # redimensionamos los tensores a las dimensiones de entrada de la red\n",
    "        tensor_panel_x = cambiar_tam(tensor_panel_x, img_ancho, img_alto)\n",
    "        # normalizamos los tensores al rango [-1,1]  \n",
    "        tensor_panel_x = norm_tensor(tensor_panel_x)\n",
    "        imgs_cargadas.append(tensor_panel_x)\n",
    "    return imgs_cargadas\n",
    "\n",
    "def cambiar_tam(input_image, ancho, alto):\n",
    "    input_image = tf.image.resize(input_image, [ancho, alto], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    return input_image\n",
    "\n",
    "def norm_tensor(tensor_entrada, range = (-1,1)):\n",
    "    tensor_entrada = range[0] + ((range[1]-range[0])*tensor_entrada / 255)\n",
    "    return tensor_entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplos de Imágenes del Conjunto de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### ejemplo de uso de las funciones auxiliares, obtenemos una pareja de imagenes de ejemplo aleatoria\n",
    "nom_img_ejemplo = LISTA_TOTAL_NOMBRES[np.random.randint(0, len(LISTA_TOTAL_NOMBRES))]\n",
    "tensor_panel_0, tensor_panel_1 = nombre_img_a_tensores(nom_img_ejemplo, paneles=2)\n",
    "\n",
    "# Configuramos los subplots\n",
    "fig, axes = plt.subplots(1, 2)  # 1 fila, 2 columnas\n",
    "\n",
    "# Mostramos la primera imagen en el primer subplot\n",
    "axes[0].imshow((tensor_panel_0 + 1) / 2)\n",
    "axes[0].set_title('Panel 0')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Mostramos la segunda imagen en el segundo subplot\n",
    "axes[1].imshow((tensor_panel_1 + 1) / 2)\n",
    "axes[1].set_title('Panel 1')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación y Configuración de Datasets para Entrenamiento y Pruebas\n",
    "creación y configuración de datasets para el entrenamiento y las pruebas. Utiliza las listas de nombres de imágenes de entrenamiento y prueba para crear datasets en TensorFlow, aplicando la función de preprocesamiento nombre_img_a_tensores a cada imagen. Los datasets se agrupan en lotes del tamaño definido por TAM_BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Crea un dataset con los nombres de archivos de entrenamiento.\n",
    "dataset_entrenamiento = tf.data.Dataset.from_tensor_slices(LISTA_NMS_ENTRENAMIENTO)\n",
    "# Aplica una función para cargar y procesar cada pareja de imágenes.\n",
    "dataset_entrenamiento = dataset_entrenamiento.map(nombre_img_a_tensores, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# Agrupa las imágenes en lotes del tamaño definido por TAM_BATCH.\n",
    "dataset_entrenamiento = dataset_entrenamiento.batch(TAM_BATCH)\n",
    "\n",
    "# Carga de las imágenes de test.\n",
    "# 2. Crea un dataset con los nombres de archivos de prueba.\n",
    "dataset_prueba = tf.data.Dataset.from_tensor_slices(LISTA_NMS_PRUEBA)\n",
    "dataset_prueba = dataset_prueba.map(nombre_img_a_tensores, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset_prueba = dataset_prueba.batch(TAM_BATCH)\n",
    "\n",
    "# Impresión de detalles de los datasets.\n",
    "print(\"Dimensiones del conjunto de entrenamiento: \", dataset_entrenamiento)\n",
    "print(\"Dimensiones del conjunto de prueba: \", dataset_prueba)\n",
    "print(\"Tipo de datos del conjunto de entrenamiento: \", dataset_entrenamiento.element_spec)\n",
    "print(\"Tipo de datos del conjunto de prueba: \", dataset_prueba.element_spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de Bloques Convolucionales\n",
    "define dos funciones para crear bloques convolucionales que se utilizan en el modelo. La función downsample crea un bloque convolucional para reducir las dimensiones de la entrada, incluyendo una capa convolucional, normalización (opcional), y activación LeakyReLU. La función upsample realiza la operación inversa, aumentando las dimensiones a través de una capa convolucional transpuesta, normalización, dropout (opcional), y activación ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# definimos un bloque convolucional de reduccion de dimensiones\n",
    "def downsample(filters, size=4, apply_batchnorm=True):\n",
    "    # usamos la semilla para que los resultados sean reproducibles\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = Sequential ()\n",
    "    # iniciamos con una distribucion normal\n",
    "    \n",
    "    # capa convolucional\n",
    "    result.add(Conv2D(filters,\n",
    "                    size,\n",
    "                    strides=2,\n",
    "                    padding='same',\n",
    "                    kernel_initializer=initializer,\n",
    "                    use_bias= not apply_batchnorm ))\n",
    "    # capa de normalizacion\n",
    "    if apply_batchnorm:\n",
    "        result.add(BatchNormalization())\n",
    "    # capa de activacion LeakyReLU (x) = max(0,x) + alpha * min(0,x)\n",
    "    result.add(LeakyReLU())\n",
    "\n",
    "    return result\n",
    "\n",
    "# definimos un bloque convolucional de aumento de dimensiones\n",
    "def upsample(filters, size=4, apply_dropout=False):\n",
    "    # iniciamos con una distribucion normal\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "\n",
    "    result = Sequential ()\n",
    "    # capa convolucional\n",
    "    result.add(Conv2DTranspose(filters,\n",
    "                    size,\n",
    "                    strides=2,\n",
    "                    padding='same',\n",
    "                    kernel_initializer=initializer,\n",
    "                    use_bias= False ))\n",
    "    # capa de normalizacion\n",
    "    result.add(BatchNormalization())\n",
    "    # capa de dropout\n",
    "    if apply_dropout:\n",
    "        result.add(Dropout(0.5))\n",
    "    # capa de activacion\n",
    "    result.add(ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# definimos el generador\n",
    "\n",
    "def Generador():\n",
    "    # La entrada tiene un shape de [None,None,3], lo que indica una imagen a color (RGB) con dimensiones variables.\n",
    "    # 'None' en las dos primeras dimensiones permite flexibilidad en el tamaño de la imagen.\n",
    "    # 'bs' denota \"batch size\", el número de imágenes procesadas en un lote.\n",
    "    inputs = tf.keras.layers.Input(shape=[None, None, 3])\n",
    "\n",
    "    # pila_reduccion contiene capas convolucionales que reducen la dimensión espacial de la imagen.\n",
    "    down_stack = [\n",
    "        downsample(64, apply_batchnorm=False), # Entrada: (bs, 256, 256, 3), Salida: (bs, 128, 128, 64)\n",
    "        downsample(128),                       # Entrada: (bs, 128, 128, 64), Salida: (bs, 64, 64, 128)\n",
    "        downsample(256),                       # Entrada: (bs, 64, 64, 128), Salida: (bs, 32, 32, 256)\n",
    "        downsample(512),                       # Entrada: (bs, 32, 32, 256), Salida: (bs, 16, 16, 512)\n",
    "        downsample(512),                       # Entrada: (bs, 16, 16, 512), Salida: (bs, 8, 8, 512)\n",
    "        downsample(512),                       # Entrada: (bs, 8, 8, 512), Salida: (bs, 4, 4, 512)\n",
    "        downsample(512),                       # Entrada: (bs, 4, 4, 512), Salida: (bs, 2, 2, 512)\n",
    "        downsample(512),                       # Entrada: (bs, 2, 2, 512), Salida: (bs, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "      # pila_aumento contiene capas convolucionales que aumentan la dimensión espacial de la imagen.\n",
    "    up_stack = [\n",
    "        upsample(512, apply_dropout=True),# Entrada: (bs, 1, 1, 512),   # Salida: (bs, 2, 2, 1024)\n",
    "        upsample(512, apply_dropout=True),# Entrada: (bs, 2, 2, 1024),  # Salida: (bs, 4, 4, 1024)\n",
    "        upsample(512, apply_dropout=True),# Entrada: (bs, 4, 4, 1024),  # Salida: (bs, 8, 8, 1024)\n",
    "        upsample(512),                    # Entrada: (bs, 8, 8, 1024),  # Salida: (bs, 16, 16, 1024)\n",
    "        upsample(256),                    # Entrada: (bs, 16, 16, 1024),# Salida: (bs, 32, 32, 512)\n",
    "        upsample(128),                    # Entrada: (bs, 32, 32, 512), # Salida: (bs, 64, 64, 256)\n",
    "        upsample(64),                     # Entrada: (bs, 64, 64, 256), # Salida: (bs, 128, 128, 128)\n",
    "\n",
    "    ]\n",
    "\n",
    "    # inicializamos la capa de salida con una distribucion normal \n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # capa de salida con una funcion de activacion tanh\n",
    "    # Entrada: (bs, 128, 128, 128), Salida: (bs, 256, 256, CANALES_SALIDA)\n",
    "    last = Conv2DTranspose(CANALES_SALIDA,\n",
    "                           kernel_size=4,\n",
    "                           strides=2,\n",
    "                           padding='same',\n",
    "                           kernel_initializer=initializer,\n",
    "                           activation='tanh') \n",
    "    \n",
    "\n",
    "    # conectamos las capas en cascada\n",
    "    x = inputs\n",
    "    skips = []\n",
    "\n",
    "    concat = Concatenate()\n",
    "\n",
    "    for reduccion in down_stack:\n",
    "        x= reduccion(x) \n",
    "        skips.append(x)\n",
    "    \n",
    "    skips = reversed(skips[:-1])\n",
    "\n",
    "    for aumento, salto in zip(up_stack, skips):\n",
    "        x = aumento(x)\n",
    "        x = concat([x, salto])\n",
    "    \n",
    "    last = last(x)\n",
    "    return Model(inputs=inputs, outputs=last)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arquitectura del discriminador\n",
    "\n",
    "def Discriminator():\n",
    "    img_entrada = Input(shape=[None, None, 3], name='input_image')\n",
    "    img_generada = Input(shape=[None, None, 3], name='target_image')\n",
    "\n",
    "    concat = Concatenate()([img_entrada, img_generada])\n",
    "\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # definimos las capas convolucionales conectadas en cascada para la reduccion\n",
    "    reduccion_1 = downsample(64, apply_batchnorm=False)(concat) \n",
    "    reduccion_2 = downsample(128)(reduccion_1)\n",
    "    reduccion_3 = downsample(256)(reduccion_2)\n",
    "    reduccion_4 = downsample(512)(reduccion_3)\n",
    "\n",
    "    capa_final = tf.keras.layers.Conv2D(filters=1,\n",
    "                                    kernel_size=4,\n",
    "                                    strides=1,\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    padding='same')(reduccion_4)\n",
    "    return tf.keras.Model(inputs=[img_entrada, img_generada], outputs=capa_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!! AVISO !!!\n",
    "debido a problemas con el guardado del modelo usando downsample y upsample, aqui está configurado un nuevo generador con las mismas características que el anterior, pero sin usar las funciones downsample y upsample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de los Modelos Generador y Discriminador\n",
    "define dos modelos clave para una Red Generativa Antagónica (GAN): el Generador y el Discriminador. El modelo Generador utiliza capas convolucionales para transformar una entrada en una imagen generada, empleando una serie de capas de reducción y aumento para procesar los datos. El modelo Discriminador evalúa pares de imágenes (una real y una generada) y decide si la imagen generada es realista. Ambos modelos utilizan capas convolucionales, normalización por lotes, y activaciones LeakyReLU y ReLU, esenciales en la arquitectura de una GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Generador():\n",
    "    entradas = Input(shape=[None, None, 3])\n",
    "\n",
    "    # Inicializador común para las capas convolucionales\n",
    "    inicializador = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # Pila de reducción\n",
    "    pila_reduccion = [\n",
    "        # Capa convolucional, sin normalización batch\n",
    "        # Entrada: (bs, 256, 256, 3) \n",
    "        Sequential([\n",
    "            Conv2D(64, 4, strides=2, padding='same', kernel_initializer=inicializador, use_bias=True),\n",
    "            LeakyReLU()\n",
    "        ]),#Salida: (bs, 128, 128, 64)\n",
    "        # Capas convolucionales con normalización batch\n",
    "        # Entrada: (bs, 128, 128, 64)\n",
    "        Sequential([\n",
    "            Conv2D(128, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()\n",
    "        ]), # Salida: (bs, 64, 64, 128)\n",
    "        # Entrada: (bs, 64, 64, 128),   \n",
    "        Sequential([\n",
    "            Conv2D(256, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()\n",
    "        ]),#Salida: (bs, 32, 32, 256) \n",
    "        # Entrada: (bs, 32, 32, 256),\n",
    "        Sequential([\n",
    "            Conv2D(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()\n",
    "        ]),#Salida: (bs, 16, 16, 512)\n",
    "        # Entrada: (bs, 16, 16, 512),\n",
    "        Sequential([\n",
    "            Conv2D(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()\n",
    "        ]),#Salida: (bs, 8, 8, 512)\n",
    "        # Entrada: (bs, 8, 8, 512),  \n",
    "        Sequential([\n",
    "            Conv2D(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()\n",
    "        ]),#Salida: (bs, 4, 4, 512)\n",
    "        # Entrada: (bs, 4, 4, 512),  \n",
    "        Sequential([\n",
    "            Conv2D(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()\n",
    "        ]),#Salida: (bs, 2, 2, 512)\n",
    "        # Entrada: (bs, 2, 2, 512),  \n",
    "        Sequential([\n",
    "            Conv2D(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            LeakyReLU()\n",
    "        ]),#Salida: (bs, 1, 1, 512)\n",
    "    ]\n",
    "\n",
    "    # Pila de aumento\n",
    "    pila_aumento = [\n",
    "        # Entrada: (bs, 1, 1, 512),   \n",
    "        Sequential([\n",
    "            Conv2DTranspose(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            ReLU()\n",
    "        ]),# Salida: (bs, 2, 2, 1024)\n",
    "        # Entrada: (bs, 2, 2, 1024),  \n",
    "        Sequential([\n",
    "            Conv2DTranspose(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            ReLU()\n",
    "        ]),# Salida: (bs, 4, 4, 1024)\n",
    "        # Entrada: (bs, 4, 4, 1024),  \n",
    "        Sequential([\n",
    "            Conv2DTranspose(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "            ReLU()\n",
    "        ]),# Salida: (bs, 8, 8, 1024)\n",
    "        # Entrada: (bs, 8, 8, 1024),  \n",
    "        Sequential([\n",
    "            Conv2DTranspose(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ]),# Salida: (bs, 16, 16, 1024)\n",
    "        # Entrada: (bs, 16, 16, 1024),\n",
    "        Sequential([\n",
    "            Conv2DTranspose(256, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ]),# Salida: (bs, 32, 32, 512)\n",
    "        # Entrada: (bs, 32, 32, 512), \n",
    "        Sequential([\n",
    "            Conv2DTranspose(128, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ]),# Salida: (bs, 64, 64, 256)\n",
    "        # Entrada: (bs, 64, 64, 256), \n",
    "        Sequential([\n",
    "            Conv2DTranspose(64, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ]),# Salida: (bs, 128, 128, 128)\n",
    "    ]\n",
    "\n",
    "    # Capa de salida final con activación tanh\n",
    "    capa_final = Conv2DTranspose(CANALES_SALIDA, 4, strides=2, padding='same', kernel_initializer=inicializador, activation='tanh')\n",
    "\n",
    "    # Conectar las capas en cascada\n",
    "    x = entradas\n",
    "    omisiones = []\n",
    "    concat = Concatenate()\n",
    "\n",
    "    for reduccion in pila_reduccion:\n",
    "        x = reduccion(x)\n",
    "        omisiones.append(x)\n",
    "\n",
    "    omisiones = reversed(omisiones[:-1])\n",
    "\n",
    "    for aumento, omision in zip(pila_aumento, omisiones):\n",
    "        x = aumento(x)\n",
    "        x = concat([x, omision])\n",
    "\n",
    "    x = capa_final(x)\n",
    "\n",
    "    return Model(inputs=entradas, outputs=x)\n",
    "\n",
    "\n",
    "\n",
    "def Discriminador():\n",
    "    img_entrada = Input(shape=[None, None, 3], name='img_entrada')\n",
    "    img_generada = Input(shape=[None, None, 3], name='img_generada')\n",
    "\n",
    "    concat = Concatenate()([img_entrada, img_generada])\n",
    "\n",
    "    inicializador = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    # Definimos las capas convolucionales conectadas en cascada para la reducción\n",
    "    # Primera capa de reducción, sin normalización batch\n",
    "    reduccion_1 = Sequential([\n",
    "        Conv2D(64, 4, strides=2, padding='same', kernel_initializer=inicializador, use_bias=True),\n",
    "        LeakyReLU()\n",
    "    ])(concat)\n",
    "\n",
    "    # Capas sucesivas de reducción con normalización batch\n",
    "    reduccion_2 = Sequential([\n",
    "        Conv2D(128, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU()\n",
    "    ])(reduccion_1)\n",
    "\n",
    "    reduccion_3 = Sequential([\n",
    "        Conv2D(256, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU()\n",
    "    ])(reduccion_2)\n",
    "\n",
    "    reduccion_4 = Sequential([\n",
    "        Conv2D(512, 4, strides=2, padding='same', kernel_initializer=inicializador),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU()\n",
    "    ])(reduccion_3)\n",
    "\n",
    "    # Capa final\n",
    "    capa_final = Conv2D(filters=1,\n",
    "                        kernel_size=4,\n",
    "                        strides=1,\n",
    "                        kernel_initializer=inicializador,\n",
    "                        padding='same')(reduccion_4)\n",
    "\n",
    "    return Model(inputs=[img_entrada, img_generada], outputs=capa_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de Coste para el Entrenamiento de Generador y Discriminador\n",
    "define las funciones de coste para entrenar los modelos de Generador y Discriminador en una Red Generativa Antagónica (GAN). Utiliza la entropía cruzada binaria para calcular la diferencia entre las distribuciones de probabilidad. La función perdida_generador evalúa la autenticidad de las imágenes generadas y la similitud L1 con las imágenes objetivo. La función perdida_discriminador evalúa la capacidad del Discriminador para distinguir entre imágenes reales y generadas. Estas funciones son cruciales para guiar el proceso de aprendizaje y optimización en las GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funciones de coste\n",
    "\n",
    "# BinaryCrossentropy es una funcion que calcula la perdida de entropia cruzada entre dos distribuciones de probabilidad\n",
    "# from_logits=True indica que la funcion de perdida espera que la salida del discriminador sea una distribucion de probabilidad\n",
    "objeto_funcion_perdida = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# definimos la funcion de coste para el generador\n",
    "def perdida_generador(disc_generated_output, gen_output, target):\n",
    "    # calculamos la perdida para las imagenes generadas que es la salida del discriminador para las imagenes generadas\n",
    "    # tf.ones_like devuelve un tensor con la misma forma que el tensor de entrada pero con todos los valores a 1\n",
    "    # disc_generated_output es la salida del discriminador para las imagenes generadas\n",
    "    perdida_img_generada = objeto_funcion_perdida(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # calculamos la perdida L1 entre la imagen generada y la imagen objetivo\n",
    "    # tf.reduce_mean calcula la media de los valores de un tensor\n",
    "    # gen_output es la imagen generada\n",
    "    # target es la imagen objetivo\n",
    "    perdida_distancia_l1 = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    # calculamos la perdida total como la suma de la perdida gan y la perdida L1\n",
    "    # Lambda es un parametro que controla la importancia de la perdida L1 en comparacion con la perdida gan\n",
    "    perdida_total_generador = perdida_img_generada + (LAMBDA * perdida_distancia_l1)\n",
    "\n",
    "    return perdida_total_generador\n",
    "\n",
    "def perdida_discriminador(disc_real_output, disc_generated_output):\n",
    "    # calculamos la perdida para las imagenes reales que es la salida del discriminador para las imagenes reales\n",
    "    # tf.ones_like devuelve un tensor con la misma forma que el tensor de entrada pero con todos los valores a 1\n",
    "    # disc_real_output es la salida del discriminador para las imagenes reales\n",
    "    #  \n",
    "    perdida_img_real = objeto_funcion_perdida(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    # calculamos la perdida para las imagenes generadas que es la salida del discriminador para las imagenes generadas\n",
    "    # tf.zeros_like devuelve un tensor con la misma forma que el tensor de entrada pero con todos los valores a 0\n",
    "    # disc_generated_output es la salida del discriminador para las imagenes generadas\n",
    "    perdida_img_generada = objeto_funcion_perdida(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # calculamos la perdida total como la suma de las perdidas para las imagenes reales y generadas\n",
    "    perdida_total_del_discriminador = perdida_img_real + perdida_img_generada\n",
    "\n",
    "    return perdida_total_del_discriminador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización de Modelos y Optimizadores\n",
    "se instancian los modelos de Generador y Discriminador definidos previamente, y se configuran los optimizadores para ambos. Se utiliza el optimizador Adam con una tasa de aprendizaje de \n",
    "2×10^−4 y un valor de beta_1 de 0.5. Estos optimizadores se aplicarán en el proceso de entrenamiento de las redes neuronales, facilitando la actualización de los pesos y mejorando el rendimiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generador = Generador()\n",
    "discriminador = Discriminador()\n",
    "optimizador_generador = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "optimizador_discriminador = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar una imagen aleatoria\n",
    "imagen_seleccionada = LISTA_TOTAL_NOMBRES[np.random.randint(0, len(LISTA_TOTAL_NOMBRES))]\n",
    "pareja_tensores = nombre_img_a_tensores(imagen_seleccionada, paneles=2)\n",
    "tensor_entrada = tf.expand_dims(pareja_tensores[0], 0)\n",
    "\n",
    "# Generar una imagen usando el Generador\n",
    "tensor_generado = generador(tensor_entrada, training=False)\n",
    "\n",
    "# elimina la normalizacion de las imagenes para que el discriminador pueda procesarlas y \n",
    "# los convierte en int\n",
    "imagen_generada = ((tensor_generado[0, ...,1])+1)*255\n",
    "# Evaluar la imagen generada usando el Discriminador\n",
    "tensor_discriminado = discriminador([tensor_entrada, tensor_generado], training=False)\n",
    "imagen_tensor_discriminado = (tensor_discriminado[0, ...,-1]+1)*255\n",
    "# Visualización\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Visualizar la imagen generada\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Imagen Generada\")\n",
    "plt.imshow(imagen_generada)\n",
    "plt.axis('off')\n",
    "\n",
    "# Visualizar la salida del Discriminador\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Evaluación del Discriminador\")\n",
    "# Ajustar la escala de colores según la salida del discriminador\n",
    "plt.imshow(imagen_tensor_discriminado)\n",
    "plt.colorbar()\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Checkpoints - puntos de control del modelo\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizador_generador=optimizador_generador,\n",
    "                                optimizador_discriminador=optimizador_discriminador,\n",
    "                                generador=generador,\n",
    "                                discriminador=discriminador)\n",
    "\n",
    "latest_checkpoint = tf.train.latest_checkpoint(RUTA_CHECKPOINTS)\n",
    "if latest_checkpoint:\n",
    "    checkpoint.restore(latest_checkpoint).assert_consumed()\n",
    "    print(f\"Restaurado desde {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"Chekpoint no encontrado, entrenamiento inicial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generamos imagenes con el generador entrenado en el checkpoint\n",
    "def generar_imagen(modelo_generador, img_prueba, img_objetivo=None, guardar_archivo=False, mostrar_imgs=False):\n",
    "    # Generamos la imagen\n",
    "    img_generada = modelo_generador(img_prueba, training=True)\n",
    "\n",
    "    # Mostramos la imagen generada\n",
    "    if mostrar_imgs:\n",
    "        plt.figure(figsize=(15,15))\n",
    "\n",
    "        titulos = ['Entrada', 'Generada']\n",
    "        lista_imgs_a_mostrar = [img_prueba[0], img_generada[0]]\n",
    "\n",
    "        # Si img_objetivo no es None, incluirlo en la visualización\n",
    "        if img_objetivo is not None:\n",
    "            titulos.insert(1, 'Realidad')\n",
    "            lista_imgs_a_mostrar.insert(1, img_objetivo[0])\n",
    "\n",
    "        # Mostrar las imágenes\n",
    "        for i, img in enumerate(lista_imgs_a_mostrar):\n",
    "            plt.subplot(1, len(lista_imgs_a_mostrar), i+1)\n",
    "            plt.title(titulos[i])\n",
    "            # Ajustar los valores de los píxeles entre 0 y 1 si es necesario\n",
    "            plt.imshow(img * 0.5 + 0.5)\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Guardar la imagen generada\n",
    "    if guardar_archivo:\n",
    "        tf.keras.preprocessing.image.save_img(guardar_archivo, ((img_generada[0] + 1) * 127.5).numpy().astype(\"uint8\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rutina de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# paso de entrenamiento\n",
    "@tf.function\n",
    "def paso_entrenamiento(tensor_entrada, tensor_objetivo):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # generamos la imagen\n",
    "        tensor_generado = generador(tensor_entrada, training=True)\n",
    "        discriminacion_tensor_generado = discriminador([tensor_generado, tensor_entrada], training=True)\n",
    "        discriminacion_tensor_objetivo = discriminador([tensor_objetivo, tensor_entrada], training=True)\n",
    "        valor_perdida_discriminador = perdida_discriminador(discriminacion_tensor_objetivo, discriminacion_tensor_generado)\n",
    "        valor_perdida_generador = perdida_generador(discriminacion_tensor_generado, tensor_generado, tensor_objetivo)\n",
    "\n",
    "        # calculamos los gradientes\n",
    "        gradientes_generador = gen_tape.gradient(valor_perdida_generador, generador.trainable_variables)\n",
    "        gradientes_discriminador = disc_tape.gradient(valor_perdida_discriminador, discriminador.trainable_variables)\n",
    "        optimizador_generador.apply_gradients(zip(gradientes_generador, generador.trainable_variables))\n",
    "        optimizador_discriminador.apply_gradients(zip(gradientes_discriminador, discriminador.trainable_variables))\n",
    "\n",
    "# rutina de entrenamiento\n",
    "# definimos la funcion de entrenamiento\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def entrenamiento(dataset, epocas):\n",
    "    \n",
    "    for epoca in range(epocas):\n",
    "        tiempo_inicio = time.time()\n",
    "        img_i = 0\n",
    "        for tensor_entrada, tensor_objetivo in dataset:\n",
    "            print(\"Epoca:\", epoca, \"Imagen:\", img_i, \"de\", len(LISTA_NMS_ENTRENAMIENTO))\n",
    "            img_i += 1\n",
    "            paso_entrenamiento(tensor_entrada, tensor_objetivo)\n",
    "            #clear_output(wait=True)\n",
    "        \n",
    "        img_i = 0\n",
    "\n",
    "        # Verifica que el directorio de salida exista\n",
    "        ruta_salida = f\"{RUTA_DATASET}/resultados_entrenamiento\"\n",
    "        if not os.path.exists(ruta_salida):\n",
    "            os.makedirs(ruta_salida)\n",
    "\n",
    "        # Genera imágenes para el entrenamiento\n",
    "        for entrada_vias , objetivo in dataset_prueba.take(25):\n",
    "            img_folder = f\"{ruta_salida}/imagen_{img_i}\"\n",
    "            if not os.path.exists(img_folder):\n",
    "                os.makedirs(img_folder)\n",
    "                \n",
    "            save_filename = f\"{img_folder}/img{img_i}_epoca_{epoca:03d}.png\"\n",
    "            generar_imagen(generador, entrada_vias, objetivo, guardar_archivo=save_filename)\n",
    "            img_i += 1\n",
    "\n",
    "        # Guardar cada n epocas\n",
    "        if (epoca + 1) % AUTOGUARDADO_EPOCAS == 0:\n",
    "            checkpoint.save(file_prefix=PREFIJO_CHECKPOINT)\n",
    "            generador.save(f'{RUTA_CHECKPOINTS}/generador_epoca_{epoca+1}.h5')\n",
    "\n",
    "        print(f'Tiempo por completar época {epoca + 1} es {time.time() - tiempo_inicio} seg\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento del modelo ajustando el numero de epocas\n",
    "\n",
    "se guarda el modelo entrenado cada N epocas en la ruta checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "entrenamiento(dataset_entrenamiento, EPOCAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de un modelo entrenado\n",
    "\n",
    "Carga de un modelo entrenado, la funcion a continuacion carga el ultimo modelo y queda listo para ser utilizado para generar imagenes dada una ruta para el archivo .png  de dimensiones 256x256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_modelo_mas_reciente(ruta_checkpoints):\n",
    "    # Expresión regular para extraer el número de época del nombre del archivo\n",
    "    regex_epoca = r\"generador_epoca_(\\d+)\\.h5\"\n",
    "\n",
    "    # Lista para almacenar los nombres de los archivos y los números de las épocas\n",
    "    modelos = []\n",
    "\n",
    "    # Recorrer todos los archivos en la carpeta\n",
    "    for archivo in os.listdir(ruta_checkpoints):\n",
    "        coincidencia = re.search(regex_epoca, archivo)\n",
    "        if coincidencia:\n",
    "            numero_epoca = int(coincidencia.group(1))\n",
    "            ruta_completa = os.path.join(ruta_checkpoints, archivo)\n",
    "            modelos.append((ruta_completa, numero_epoca))\n",
    "\n",
    "    # Verificar si se encontraron modelos\n",
    "    if not modelos:\n",
    "        print(\"No se encontraron modelos .h5 en la ruta proporcionada.\")\n",
    "        return None\n",
    "\n",
    "    # Ordenar los modelos por número de época (el más reciente primero)\n",
    "    modelos.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Cargar y devolver el modelo más reciente\n",
    "    modelo_mas_reciente = modelos[0][0]\n",
    "    print(f\"Cargando el modelo más reciente: {modelo_mas_reciente}\")\n",
    "    return tf.keras.models.load_model(modelo_mas_reciente)\n",
    "\n",
    "\n",
    "# Cargar el modelo generador más reciente\n",
    "modelo_generador = cargar_modelo_mas_reciente(RUTA_CHECKPOINTS)\n",
    "\n",
    "# Elegir una imagen de prueba\n",
    "lista_imagenes_de_prueba = os.listdir(RUTA_IMG_PRUEBA)\n",
    "imagen_prueba_entrada = lista_imagenes_de_prueba[3]\n",
    "print(\"Elegimos la imagen:\", imagen_prueba_entrada)\n",
    "\n",
    "# Preparar el tensor de entrada\n",
    "tensor_entrada = nombre_img_a_tensores(imagen_prueba_entrada)\n",
    "tensor_entrada = tf.expand_dims(tensor_entrada[0], 0)\n",
    "\n",
    "# Utilizar generar_imagen para visualizar la salida\n",
    "generar_imagen(modelo_generador, tensor_entrada, mostrar_imgs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluacion del modelo\n",
    "#### Generacion de imagenes dados checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## video mp4\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def imagenes_a_video(ruta_carpeta, ruta_salida, fps=30):\n",
    "    dir_trabajo = os.getcwd()\n",
    "    ruta_absoluta_carpeta = os.path.join(dir_trabajo, ruta_carpeta)\n",
    "    \n",
    "    if not os.path.exists(ruta_absoluta_carpeta):\n",
    "        print(f\"La carpeta {ruta_absoluta_carpeta} no existe.\")\n",
    "        return\n",
    "\n",
    "    archivos = [f for f in os.listdir(ruta_absoluta_carpeta) if os.path.isfile(os.path.join(ruta_absoluta_carpeta, f))]\n",
    "    print(f\"Número de archivos en {ruta_absoluta_carpeta}: {len(archivos)}\")\n",
    "\n",
    "    archivos.sort()\n",
    "    \n",
    "    # Leer la primera imagen para obtener las dimensiones\n",
    "    img = cv2.imread(os.path.join(ruta_absoluta_carpeta, archivos[0]))\n",
    "    altura, ancho, _ = img.shape\n",
    "    \n",
    "    # Establecer la configuración para el video de salida\n",
    "    ruta_absoluta_salida = os.path.join(dir_trabajo, ruta_salida)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(ruta_absoluta_salida, fourcc, fps, (ancho, altura))\n",
    "\n",
    "    for archivo in archivos:\n",
    "        ruta_img = os.path.join(ruta_absoluta_carpeta, archivo)\n",
    "        img = cv2.imread(ruta_img)\n",
    "        out.write(img)\n",
    "        \n",
    "    out.release()\n",
    "    print(f\"Video guardado en {ruta_absoluta_salida}\")\n",
    "\n",
    "# Ejemplo de uso en varias carpetas, un video por carpeta con las imágenes\n",
    "for x in range(25):\n",
    "    ruta_carpeta = f\"dataset_geo_col_256/training_outputs/imagen_{x}\"\n",
    "    ruta_salida = f\"dataset_geo_col_256/training_outputs/animacion_batch_500_epocas_200_img_{x}.mp4\"\n",
    "    imagenes_a_video(ruta_carpeta, ruta_salida, fps=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptos vinculados para entender antes de aplicar la Pérdida de Entropía Cruzada Sigmoide\n",
    "Probabilidad y Estadísticas: Comprensión de la teoría básica de probabilidad y conceptos estadísticos.\n",
    "Logaritmos: La función de pérdida utiliza logaritmos, por lo que una comprensión básica es esencial.\n",
    "Cálculo: Específicamente, comprensión de derivadas y gradientes para la optimización.\n",
    "Algoritmos de Optimización: Familiaridad con algoritmos como el Descenso de Gradiente que se utilizan para minimizar la función de pérdida.\n",
    "Redes Neuronales: Comprensión básica de qué son las redes neuronales y cómo funcionan, incluida la propagación hacia adelante y hacia atrás.\n",
    "Funciones de Activación: Comprensión de qué son las funciones de activación, con un enfoque en la función sigmoide.\n",
    "Funciones de Pérdida: Comprensión general de qué son las funciones de pérdida y por qué se utilizan.\n",
    "Clasificación Binaria: Comprensión de problemas de clasificación, específicamente clasificación binaria.\n",
    "Operaciones de Tensor: Como estás trabajando con imágenes, entender cómo manipular tensores (matrices multidimensionales) es crucial.\n",
    "Redes Neuronales Convolucionales (CNN): A menudo se utilizan en tareas relacionadas con imágenes, por lo que entender su arquitectura y función es beneficioso.\n",
    "Redes Generativas: Dado que mencionaste redes generativas, entender las Redes Antagónicas Generativas (GAN) u otros modelos generativos sería útil.\n",
    "Retropropagación: Comprensión de cómo se calculan los gradientes y se actualizan los pesos durante el entrenamiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
